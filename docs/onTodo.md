<small><p>&nbsp;
<a name=top></a>
<table><tr> 
<td><a href="/README.md#top">home</a>
<td><a href="/ROADMAP.md">roadmap</a>
<td><a href="http:github.com/timm/tested/issues">issues</a>
<td> <a href="/LICENSE.md">&copy;2022,2023</a> by <a href="http://menzies.us">tim menzies</a>
</tr></table></small>
<img  align=center width=600 src="/docs/img/banner.png"></p>
<p> <img src="https://img.shields.io/badge/task-ai-blueviolet"><a
href="https://github.com/timm/tested/actions/workflows/tests.yml"> <img 
 src="https://github.com/timm/tested/actions/workflows/tests.yml/badge.svg"></a> <img 
 src="https://img.shields.io/badge/language-lua-orange"> <img 
 src="https://img.shields.io/badge/purpose-teaching-yellow"> <a 
 href="https://zenodo.org/badge/latestdoi/569981645"> <img 
 src="https://zenodo.org/badge/569981645.svg" alt="DOI"></a></p>

# not chekcing code

using code genratorswithout checking the results

"whtntnehy are sworn,g are so confiently wrong you can never tell"

# checking code too much

wating too much time checking code

not low level. high level

bugs are soclail.  traciking down one highlivel issue can uncover dozens more.

# Random notes, discarded text

Softare engineering has changed. In this era of generative AI, the role
of software engieenrs is less about cretating, and more about critiquing, code.
In this new one, someone or something else will propose a system and your
job is to write the tiny scripts that check the bigger system.

# using all data

most statues not reached.
fear: what aout the errors in thiss tates toua re ignoring. 
well, that a hard problem so lets offer asomewhat easier solution.
lets ship our systems with detectors of "usyal behavour" and red lights
that start flashing when new inputs or outputs are outside of the range
of "usual". and lets talk abiyt cotril structure that increase the odds
that te system will keep runnng within its known space where there is must
expeience, and must approval, of the outputs generated by the inputs.

b i
Our automated tools lets us explore the world better,  so lets explore it for good.   In this class, your goal as automated software engineers
  use your automated tools to find better options


Its all about choice
- Choice and power
- and tools to better support better choices
   
   Computer science is no more about computers than astronomy is about telescopes.-- Edsger W. Dijkstra. 
   <img height=175 src="https://mathshistory.st-andrews.ac.uk/Biographies/Dijkstra/thumbnail.jpg">
   (e.g. "XAI Tools in the Public Sector: A Case Study on Predicting Combined Sewer Overflows" 
     Maltbie et al. FSE'21   DOI:https://doi.org/10.1145/3468264.3468547


|                                     Freeman Dyson                                     |                                                Kimberlé Crenshaw [^crenshaw] [^coaston]                                                 |                        Pablo Picasso                         |                            Susan Sontag                            |
|:-------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------:|:------------------------------------------------------------------:|
| Technology without morality is barbarous;<br>morality without technology is impotent. | The goal  (....)  should be to facilitate the inclusion of marginalized groups for whom it can be said: "When they enter, we all enter. | Computers are stupid... they only give us answers[^picasso]. | The only interesting answers are those that destroy the questions. |
| <img height=125 src="https://upload.wikimedia.org/wikipedia/commons/0/03/Freeman_Dyson_%282005%29.jpg"> |                                              <img height=125 src="/docs/img/crenshaw.png">                                              | <img width=600 src="https://i.pinimg.com/originals/2c/97/b4/2c97b421c756a19468b58546202160e1.jpg"> | <img src="https://upload.wikimedia.org/wikipedia/commons/f/f2/Susan_Sontag_1979_%C2%A9Lynn_Gilbert_%28headshot%29.jpg" height=125> 


[^coaston]: The intersectionality wars By Jane Coaston, jane.coaston@vox.com. Updated May 28, 2019.  https://www.vox.com/the-highlight/2019/5/20/18542843/intersectionality-conservatism-law-race-gender-discrimination


[^crenshaw]: Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics.  Kimberle Crenshaw.  The University of Chicago Legal Forum 140:139-167 (1989)


[^picasso]: From [MFA
Masterworks](https://news.masterworksfineart.com/2018/10/31/pablo-picasso-and-cubism#:~:text=In%20collaboration%20with%20his%20friend,how%20he%20achieved%20this%20goal.):
"Pablo Picasso pioneered the Cubism movement, a revolutionary style
of modern art that Picasso formed in response to the rapidly changing
modern world.   Picasso wanted to emphasize the difference between
a painting and reality.Cubism involves different ways of seeing,
or perceiving, the world around us. He felt that we do not see an
object from one angle or perspective, but rather from many angles
selected by sight and movement. As a result of this belief, Cubism
became about _how_ to see an object or figure rather than _what_ the
artist was looking at."


<hr>
<h2> gonna b so much FUNNNN !! </h2>
<img align=right width=400 src="http://se16.unbox.org/.worksite/img/elearning_image1.jpg">


Y'all yearning to be learning? Me too!


Education is the not the filling of a pail, but the lighting of a fire.   
-- W.Yeats


If the world merely lived up to our wildest dreamings, what a dull place it would be. Happily...   
-- Me


Learn why the world wags and what wags it. That is the only thing which the mind
can never exhaust, never alienate, never be tortured by, never fear or distrust,
and never dream of regretting. Learning is the only thing for you. Look what a
lot of things there are to learn.  
-- T.H. White (The Once and Future King)


## Grad subject structure (sample)


(Note: often can be taken in any order-- see class syllabus to check if any restrictions apply.)


<img width=900 src="http://se16.unbox.org/_img/subjects.png">


## About NC State SE


We're number one! Nearly!


<a href="http://csrankings.org/"><img width=900 src="etc/img/csrankings.png"></a>


Largest percent female faculty in CS  in the country:


<img width=900 src="https://www.csc.ncsu.edu/enews/images2/features/1952-large.jpg">


If you want to know our SE work:


<a href="https://www.amazon.com/Perspectives-Data-Science-Software-Engineering/dp/0128042060"><img 
width=250 src="http://menzies.us/img/perspectivesBook.jpg"></a>
<a href="https://www.amazon.com/Art-Science-Analyzing-Software-Data/dp/0124115195"><img 
width=250 src="http://menzies.us/img/asdbookCover.png"></a>
<a href="https://www.amazon.com/Sharing-Data-Models-Software-Engineering/dp/0124172954"><img 
width=250 src="http://menzies.us/img/shareBookCover.png"></a> 


## About me


http://menzies.us


<img width=300 src="https://user-images.githubusercontent.com/29195/131709868-4e2c7444-0e37-4a71-bd47-b171bd2679f4.png">


## Defect Prediction and Clustering (a detailed example)


[Papakroni](https://researchrepository.wvu.edu/cgi/viewcontent.cgi?article=4403&context=etd) argued that, for that purpose,
you do not need to show models... just insightful samples from the domain.


For example, here's a summary of [POI-3.0](https://zenodo.org/record/322443#.YS-obNNuc2k). The original data set is reduced from 442 rows and
21 columns to 21 rows and 6 columns.


![image](https://user-images.githubusercontent.com/29195/131706893-d46a785a-5ce1-4999-8e08-b770d8728ecb.png)


|name | what | notes |
|---| ------|--|
|lcom3| cohesion| if m,a are the number of methods,attributes in a class number and µ(a) is the number of methods accessing an attribute,then _lcom3 = (mean(µ(a)) - m)/ (1 - m)_ |
|ce  | efferent couplings| how many other classes is used by the specific class|
|rfc |response for a class | number of methods invoked in response to a message to the object.|
|cbm |coupling between objects |total number of new/redefined methods to which all the inherited methods are coupled|
|loc |lines of code | |
|defect| defect| Binary class. Indicates whether any defect is found in a post-release bug-tracking system. |


And we know how to change these


![image](https://user-images.githubusercontent.com/29195/131706567-b348b940-c7c3-4a2f-b20b-3303e036b4db.png)


![image](https://user-images.githubusercontent.com/29195/131708447-c0f0c4af-31e9-4389-acd6-d438b9bb835b.png)


Btw, see the colors?
- One surprisingly [good defect predictor](https://home.cse.ust.hk/~hunkim/papers/nam-ase2015.pdf)
is "count how often an example has attribute values falling into the worst part of each column" (e..g which side of the emdian your fall).
- And this does not even need clss labels to impement.
  -  Defeats most other unsupervised methods (at least, on SE data). See [IVSBC in Fig8](https://yanmeng.github.io/papers/JSS203.pdf).
- And recently we've found it [useful  to look at just a few (2.5% ) of the data labels](https://arxiv.org/pdf/2108.09847.pdf)
   to refine  what we mean by "worst half"
 Goal: repeat until no crap: cut the crap


<img src="https://user-images.githubusercontent.com/29195/131719589-f259227c-562c-4249-956b-4ba9c62f6bfb.png" align=right width=400>


"Every <strike>block of stone</strike> has a <strike>statue</strike> signal inside it and it is the taks of the scultpro to discover it. "       
-- <strike>Michelangelo</strike> some bald guy


“Perfection is achieved when there is nothing left to take away.”        
 -- Antoine de Saint-Exupéry 


"Less, but better."      
-- Dieter Rams


"In most applications examples are not spread uniformly throughout the instance space, but are concentrated on or near
a lower-dimensional manifold. Learners can implicitly take
advantage of this lower effective dimension."      
-- Pedro Domingoes


<br clear=all>
<img src="https://user-images.githubusercontent.com/29195/131719792-eca77ca2-b7ee-436a-9e6d-cfbe521aa157.png"  width=900>


## Motivating Examples


### Example1:   effort estiamtion


Question: is highly complex software slower to build?    
Answer: the question is irrelevant (at some sites)


![image](https://user-images.githubusercontent.com/29195/131710162-3f3869f3-95cb-4f97-93c9-e7b7e013bda6.png)


So what else can we throw away.


![image](https://user-images.githubusercontent.com/29195/131708345-b3e25f09-c4c2-4a34-8979-b96a178d26e5.png)


![image](https://user-images.githubusercontent.com/29195/131708361-1ad1e4ec-712a-4454-a90d-4d6e8a156a50.png)


discretqion: volumnes not points
[Discretization: An Enabling Technique](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/liu1-2.pdf)
Huan Liu
Farhad Hussain 
Chew Lim Tan 
Manoranjan Dash
Data Mining and Knowledge Discovery, 6, 393–423, 2002


Ramírez-Gallego, Sergio & García, Salvador & Mouriño-Talín, Héctor & Martinez, David 
and Bolón-Canedo, Verónica & Alonso-Betanzos, Amparo & Benítez, José & Herrera, Francisco. (2015). 
[Data discretization: Taxonomy and big data challenge](https://www.researchgate.net/publication/284786447_Data_discretization_Taxonomy_and_big_data_challenge)
Discovery. 6. n/a-n/a. 10.1002/widm.1173. 


Example of crazy high dimensional effects:
 - A large N-dimensional unit sphere (radius=1) has finitely small volume.
 - V(2)=  circle = &pi;r<sup>2</sup>
 - V(3)= sphere = 4/3&pi;r<sup>3</sup>
 - V(n) = hypersphere = 2&pi;r<sup>2</sup> \* V(n-2) / n
   - at r=1,n=6, V(n) > V(n-2)
   - but after r=1,n=7  V(n) &lt; V(n-2)
   - Why? well for unit spheres (where r=1) L2 says ((a1-a2)<sup>2</sup> +(b1-b2)<sup>2</sup> + (c1-c2)<sup>2</sup> + .... )<sup>1/2</sup>
     - Q: How to  maintain r=1 as the number of dimensions icnreasea?
     - A: Minimize all the gaps (a1-a2), (b1-b2), (c1-c2), etc
     - So for constant radius, as dimensions grows, the gap betweeen examples has to shrinl
 - Practical consequences: as models get more complex, the space of relevant (i.e. nearby) examples gets vanishingly small
   - Good news: only need to search in nearby region for relevant data
   - Bad news: models are either low-dimensional or can't be modeled (not enough relevant data)
   - Better news: the models that can be modeled conform to the manifold assumption.
 
 Q: How to find those lower dimensions?    
 A: Let them find you.
- multiple times, take randoms steps across the space
 
 - technique used to reduce the dimensionality of a set of points
-  known for their power, simplicity, and low error rates when compared to other methods
- if n randomly selected dimension say you are similar to something else
    - then you are probably similar


 - "Fortunately
 in most applications examples are not spread uniformly throughout the instance space, but are concentrated on or near
a lower-dimensional manifold. "
 - "For example, k-nearest neighbor works quite well for handwritten digit recognition even
though images of digits have one dimension per pixel, because the space of digit images is much smaller than the
space of all possible images." 
- "Learners can implicitly take
advantage of this lower effective dimension"
 
 


<img src="https://ih1.redbubble.net/image.3318871488.2254/ur,pin_small_front,square,1000x1000.jpg">


secret of big test is small test


-Lional assurance cases
- timm's test of cinalical instrucment
- teratex
- lexis nexis : small egs
- zhe's support vectors


small is good
- valderi's rule
    -repgrid rule


### Regular expressions 
_Problem:_  build "little languages": parsers for short-cut specialized languages


_Solution:_ Regular expressions [^Cox07] [^Thom68] 
<img src="/etc/img/fsm.png" align=right width=400>
are a text form of a state-transition diagram
with some special `accept` state. If some stream of characters can
walk the transitions and arrive at the accept state, then we say
the pattern matches the characters.


_Example:_  see lib.lua#settings' processing of [101.lua#the](/src/101.lua)
 
### Incremental Learning
_Problem:_ learn a summary from an infinite stream of data.


_Solution #1:_ (for symbols): e.g. [101.lua@SYM](/src/101.lua)


_Solution #2:_ (for numerics)
A one-pass Sd via Welford's algorithm [^Welford62] (see [101.lua#NUM](/src/101.lua).
This algorithm is much less prone to loss of 
precision due to catastrophic cancellation, 


_Solution #3:_ (for numerics). See reservoir sampling


### Reservoir Sampling
_Problem:_ Summarize an infinite stream of numbers.


_Solution:_
A reservoir sampling[^ResXX]  is a family of randomized algorithms for randomly choosing k samples from a list of n items,i
where n is either a very large or unknown number. Typically n is large enough that the list does not 
fit into main memory. For example, a list of search queries in Google and Facebook.


Using the reservoir sampler, we can compute standard deviation as follows.
Diversity
can be computed by looking at the difference between large numbers and small numbers in an array.
Lets use that:
- When the reservoir changed, set a flag to `false`.
- When you want the reservoir, sort it and set flag to `true`.
  - overtime, you will sort, less and less 
- We know that
  ±2,2.58,3 standard deviations covers 66,90,95%, 
    <img src="/etc/img/128.png" align=right width=300>
    of the mass.  
- So one standard deviation is (90-10)th divide by 2.58 times σ. 


_Example:_ [SOME](/src/101.lua)


Prob
- Stochastic sampling can tame hard problems (see reservoir sampling and 101.lua@SOME))
- Lehmer [^Lehmer69] (a.k.a. Park-Miller) a pseudorandom number algorithm  for generating 
  a sequence of numbers whose properties approximate the properties of sequences of random numbers. 
  The sequence is not truly random, because it is completely determined by an initial value, 
  called the `seed`. By resetting the seed, the entire "random" sequence can be recreated.
  - Control your seed (or else)
- Fisher Yates shuffle [^Fisher38], randomizing in linear time algorithm for sorting a list of numbers.
  To save memory, it sorts in the same space as the array.
- NUM/SYM middle point is mode/mean
- NUM/SYM div(diversity) is standard deviation or entropy
- Shannon entropy [^Shannon48] <img align=right width=300 src="/etc/img/shannon.png">
  Many ways to define it, but consider it the effort required to recreate a signal.
  Given a bit stream of size `n` and two structures using `n1` then `n2` bits at probability
  $p_1=\frac{n_1}{n}$ and
  $p_2=\frac{n_2}{n}$ and
  we hunt for these via a binary chop, then that effort is
  $$-\sum_i p_i \log_2(p_i)$$


[^Cox07]:      [Regular Expression Matching Can Be Simple And Fast (but is slow in Java, Perl, PHP, Python, Ruby, ...)](https://swtch.com/~rsc/regexp/regexp1.html), 
  Russ Cox rsc@swtch.com, January 2007
[^deMo1718]:   Schneider, Ivor (2005), "Abraham De Moivre, The Doctrine of Chances (1718, 1738, 1756)", 
  in Grattan-Guinness, I. (ed.), Landmark Writings in Western Mathematics 1640–1940, Amsterdam: Elsevier, pp. 105–120, ISBN 0-444-50871-6.
[^dou95]: James Dougherty, Ron Kohavi, and Mehran Sahami. 1995. 
  [Supervised and unsupervised discretization of continuous features](https://ai.stanford.edu/~ronnyk/disc.pdf)
  In Proceedings of the Twelfth International Conference on International Conference 
  on Machine Learning (ICML'95). Morgan Kaufmann Publishers Inc., San Francisco, 
  CA, USA, 194–202.
[^Fisher38]:   Fisher, Ronald A.; Yates, Frank (1948) [1938]. Statistical tables for biological, agricultural and medical research (3rd ed.). 
  London: Oliver & Boyd. pp. 26–27. OCLC 14222135. 
[^Lap1812]:    Pierre-Simon Laplace, Théorie analytique des probabilités 1812, “Analytic Theory of Probability"
[^Lehmer69]:   W. H. Payne; J. R. Rabung; T. P. Bogyo (1969). "Coding the Lehmer pseudo-random number generator" (PDF). 
  Communications of the ACM. 12 (2): 85–86. doi:10.1145/362848.362860
[^ResXX]:      Bad me. I can recall where on the web I found this one.
[^Shannon48]:  Shannon, Claude E. (July 1948). "A Mathematical Theory of Communication". Bell System Technical Journal. 27 (3): 379–423. 
  doi:10.1002/j.1538-7305.1948.tb01338.x. hdl:10338.dmlcz/101429. 
  <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">(PDF)</a>
[^Thom68]:     Ken Thompson, “Regular expression search algorithm,” Communications of the ACM 11(6) (June 1968), pp. 419–422. 
  http://doi.acm.org/10.1145/363347.363387 <a href="https://www.oilshell.org/archive/Thompson-1968.pdf">(PDF)</a>
[^Welford62]:  Welford, B. P. (1962). "Note on a method for calculating corrected sums of squares and products". Technometrics. 4 (3): 419–420. doi:10.2307/1266577. JSTOR 1266577.

# treating code as true and finished

no as woething tat continally needs to be tamed

## not testing code

## not configuring code

# bad smells1
code too long: using 1000 lines when 100 will do

not having a high-level api

not documentiong a high-level api

not having acommand line


# On Coding
Idioms for useful code

## Env
- tweak your ide
- split your screen (connects to tdd)

## NM: numerical methods


- handle missing values, at the lowest level
- no divides without handling div/0
- evaluate things via multiple trials,
  - where training data is differeremt to test data
  - where random number seed in different each trial
- define default seed
- accept seed from command line
- reset seed before each new experiment
- when studying results, compare distributions seen in multiple trials
  - apply both effect size and significance tests
- no magic numbers. all tunetables in a global config var
- keep you intermediaries logs, yo will use them again
- where possible, divide long runs into many shorter section such
  that if some fail, then others can still finish
  - dump log data often, along the way
  - don't wait till the end else you lose a day of commutation by some
    divide by zero error that happens in the 23rd hour.
- show the summary, peek inside 
  - e.g. not just F1 but prec/recall
  - e.g. not ranks of rows, but also some of the raw row data


## LE: Less is More
1. Most functions are v.short (just a few lines, less than 100 characters wide).


## FP: Functional programming


> "first-class values":  a function is a value 
with the same rights as conventional values like numbers and strings. 
    Functions can be stored in variables (both global and local) and in tables, 
    and can be passed as arguments, and can be returned by other functions.


1. Useful for defining a test library (see  101.lua#eg)
2. Useful for callbacks (see lib.lua#csv). 
3. Useful for collecting results of an iteration (see lib.lub#map in lib.lua#o)
4. Writing function that return functions (see lib.lua#lt)


## TE: Test suite
1. Do you have half a dozen tests per person working on the project per week of work?
2. Can all the tests be run in batch?
3. From the command line can you run just one test?
4. If a test fails and crashes, can the rest of the tests still run (hint try:except:)


## SO: Source control
1. Is your code in some version control system?
2. Everyday you write code, does some branch get updated?
3. Is the test suite <a href="https://github.com/timm/tested/actions/workflows/tests.yml">triggered by each new commit</a>?
4. Do you have an automated build system (Make, Ant, Maven, Cargo, Flutter, Elm, etc etc etc) for all the tedious details.
5. Is the build system included in the documentation?

## Config is code
- add config files to version control
    e.g. etc/h
- downside: everyone has to use the same cofnig
    - upside: everyone has to use the same config (standards, no local boundaries)
    - fix: end of shared config does `[ -f "$HOME/.mylocalsettings" ] && . $HOME/.mylocalsettings`
        - i.e. allow local patches
        - but how do individuals do version control on their own config?
          - private repo "mystuff" then `ln -sf FULLPATHtoMYSTUFF/etc/.mylocalsettings`  (for auto updates each time you check our :mystuff")
            or `cp ME .localsettings` for manual updates whenever you thinka but it
        - try to keep local setting much smaller than shared (exception: anythi to do with themese, color, font choice in your ide)

## DI: Data Independence
1. Your internal model is isolated from I/O operations 
   - When reading csv files, conversions  from strings to types happens once, 
     and once only, before data is loaded into your model
   - All my file I/O routines are isolated (in lib.lua#csv)


## DD: Dialog independence
1. In the guts of your code, no direct writes to "print" but rather to some `log` function that may or may not write to the screen.
2. Can you turn off all logging (no log string generation, nothing logged/printed anywhere)?


## Ab: Abstraction
1. Using try:catch, try:except, (Lua) pcall,
  - See `pcall` in `run1`
2. You writing your own iterators ? 
  - e.g. lib.lua#csv calls `fun` for every row in a csv file


## OO: Object-oriented programming


1. Are you using polymorphism? (same name, different methods);
2. Are you using inheritance (consider doing less of that)?
  - [Hatton98](http://www.cs.kent.edu/~jmaletic/cs69995-PC/papers/Hatton98.pdf)
  - [Jabangwe14](https://www.wohlin.eu/emse14b.pdf)
  - [Al Dallal18](https://www.computer.org/csdl/journal/ts/2018/01/07833023/13rRUwkxc76)
  - [Stop Writing Classes](https://www.youtube.com/watch?v=o9pEzgHorH0)
3. Do your objects have customized create function? (e.g. 101.lua#SYM:add)
4. Do your objects have customized sort functions? (e.g. Lua __lt)
5. Do your objects have customized print functions (e.g. Lua __tostring)


## DSL DSL
1. Refactoring, on steroids.
2. Common processing, rewritten as a massive shortcuts
  - e.g. regular expressions
    - see lib.lua#settings' processing of 101.lua#the
  - e.g. help string to options 101.lua#settings


## Pa: Packaging
1. N-1 globals better than N.
2. What are you exporting?
   - See [Python tips](https://stackoverflow.com/questions/26813545/how-to-structure-a-python-module-to-limit-exported-symbols)
3. Are you using nested functions (to  hide or  clarify tedious details)
   - e.g. lib.lua#'function want'.


## Sh: Sharing
1. Code released under some license that enables sharing.
2. Project has a web site.
   - Note: my "web" site is markdown files that share the first para of "/README.md"
   - So my web site "build" system is about 10 lines of code in "/Makefile"
3. Repo includes test data, documentation, test suite results.
   - e.g. for Python pydoc and [sublime](https://menzies.us/sublime/sublime.html)
   - e.g. for Lua, [ldoc](https://stevedonovan.github.io/ldoc/manual/doc.md.html)
   - e.g. for Lua, [alfold](https://github.com/timm/tested/blob/main/docs/alfold.md)
4. Code routinely explored by  static code analysis tools (which can be very  simple e.g. 
    syntastic or very complex and slow to run e.g. your model checker of choice)?
5. Code follow standard formatting conventions:
   - e.g. Python Flake8 is a popular lint wrapper for python. Under the hood, 
     runs the `pep8` style checker,
     `pyflakes` for checking syntax,
     `mccabe` for checking complexity
6. Does your code support short release cycles 
   (no standard test is slow, really slow things are explored for optimization)
7. Does your code have zero internal boundaries 
   (e.g. everyone uses same tools, config files for those tools in repo)


# Documentation
- Do you have (at least) your public functions and classes documented?
- Can you generate doco from comments and type hints in the source code?
- Does your code follow any well-known patterns? Does the doco mention those patterns?


## Settings
- Do you have settings global?
- Can the settings be changed from the command line?
- From the command line, can you get a print of the help text?
- From the command line, you set the random number seed?


## Packages/Modules
- In your package, have you tried using fewer globals?
- Does your code pollute the global space?
- Does your package return a subset of the code in your package?


## Pipes
- Can you accept input from standard input?
- Are your errors written to standard error?
- Is your default output to standard out?


## Reuse
- Have you used your functions/objects for at least three different purposes? (e.g. NB, NN, DT all need DATA, NUM, SYM, etc)


## Simulation


- As above: Can you turn off all logging (no log string generation, nothing logged anywhere)?
- Does your code store its random number seed?
- Are your random numbers reset to the seed before each run? 


## TDD


- red
- green
- refactor
  - rule of three
  - YAGNI 
  - forever stunned at how small is the useful core of larger systems
  - the need for smalller code
- tests in different files


### Some High-Level Terms 


|What | Notes|
|----:|:------|
|Theorem provers | explores  a set constraints defined in $B$|
| Data miners |   tools for dividing up $X$ and/or $Y$|
| Optimizers |  methods for jumping between better and worse $X$ and/or $Y$ |
| Explanation| tools are methods for generating a useful succinct summary from all that was learned from the theorem provers + optimizers + data miners?  Here, _useful_ means if that summary is applied  to the data, then some $m \ll N$ good examples will be selected (and "good" means "has good $Y$ values).|


### Other Lower-Level Terms


|What | Terms|
|----:|:------|
|supervised inference|  At least one _Y_ value |
| regression | usually, for single numeric goals|
|classification | single symbolic goal|
|Semi-supervised learning|  When  we have many examples but only a small subset have $Y$ values... in which case we can do things like spreading out the available $Y$ values over clusters within the $X$ values.|
| Unsupervised learning |  When we have no $Y$ values... in which case we can do things like _cluster_ the $X$ variables then ask humans people to offer comments on each cluster.  To reduce labeling do a recursive binary clustering of the data down to leaf clusters of size $\sqrt{N}$. Then just label the median point of these $\sqrt{N}$ clusters.|
|Contrast learning | Finding a minimal difference between clusters.
|  Single goal optimization| single goal|
| Multi goal optimization | up to 3 goals|
| Many goal optimization | 4 or more|
|Hyper-parameter optimization| Optimization through $Z,Y$ space|


One way to build optimizers is to: 
- cluster on $X$
- sort each cluster of their average $Y$ values
- generate clusters between clusters with worse and better $Y$ values|


[^many]: Aurora Ramírez, José Raúl Romero, Sebastián Ventura,
A survey of many-objective optimisation in search-based software engineering,
Journal of Systems and Software,
Volume 149,
2019,
Pages 382-395,
ISSN 0164-1212,
https://doi.org/10.1016/j.jss.2018.12.015.
https://www.researchgate.net/publication/329736475_A_survey_of_many-objective_optimisation_in_search-based_software_engineering


beyond standard testing (other testing) rep rule
## asdas


<a href="/etc/img/scope2.png"><img width=400 align=right src="/etc/img/scope2.png"></a>
First, astronomers  learn how to grind lens.  Next, they use those
lens to explore the universe.


So, now that SE people know how to match their brackets, it it time
to table our next challenge. How can we help
people use our software tools to better
explore (and change) the world around us?


Our answer is based on an idea from the 1990s[^me96]: 
to test "it", you have to run "it". To say that another way,
a properly
implemented "test" engine is also an "inference" engine.
And if we do it that way, then we have a unified framework
for both testing  and execution.


For example, 
when SE developers explore software, it is called "testing". When
non-developers explore software it is called many things such as
"acceptance testing", "auditing", etc.  By refactoring those two
approaches, and adding in some AI, we can implement a kind of
incremental agile acceptance process where developers, and the
others, can use lessons from past tests to improve subsequent
versions.


[^me96]: [On the Practicality of Abductive Validation](https://d1wqtxts1xzle7.cloudfront.net/1189844/57jb73rfr4wygz2.pdf?1425075060=&response-content-disposition=inline%3B+filename%3DOn_the_Practicality_of_Abductive_Validat.pdf&Expires=1671930452&Signature=BApU0XRhl8BjEdE0UaGjG2xWjopeKj9MaNb~UYbfJIe5gLEfpynx08Usk-5ga~cGh9kRwk6vRPdZ1iRVJeVZO3KJ5oxsFXgcsex~iay0uNQBk-H43EKm5TNKRU5SfXWFs~re9erwkOUT7XvIwWGjdwHPCtZo3AvsAbhdkOcu~GekEPA1Kh9mpra0W4EzSisMmRj1iWld8O3iYrXq16etQ1NVaPsfdCQ-46ck6grkpjdttvE04W2HSowijrAdJAaKssBBHJ5w1AAESVAzpnvMvSDq732Gyre7vDJocjRtgF-DWDJSpFVzx3DXrbSdcxj1Z3rbCEsJQNgY8Fp0Qbffsg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)
         Menzies, Tim. ECAI. Vol. 96,  1996.


Q1) But wait, is this an AI subject or a SE subject?   
A1) Yes.
AI is software[^me]  and as such it needs engineers who have the
experience needed to revise and refactor and improve it[^mart].


Q2) Is this an AI subject or a requirements engineering subject?    
A2) Yes. Requirements and testing are both iterative processes where we
try to learn the most, using the least samples.


[^me]: T. Menzies, 
       ["The Five Laws of SE for AI"](https://github.com/timm/timm.github.io/blob/master/pdf/20FiveLawsSE4AI.pdf),
       in IEEE Software, 
       vol. 37, no. 1, pp. 81-85, Jan.-Feb. 2020, doi: 10.1109/MS.2019.2954841.<br>


[^mart]:  Silverio Martínez-Fernández, Justus Bogner, Xavier Franch, Marc Oriol, Julien Siebert, Adam Trendowicz, Anna Maria Vollmer, and Stefan Wagner. 2022. 
        [Software Engineering for AI-Based Systems: A Survey](https://dl.acm.org/doi/pdf/10.1145/3487043)
        ACM Trans. Softw. Eng. Methodol. 31, 2, Article 37e (April 2022), 59 pages. https://doi.org/10.1145/3487043


Is all that too complex for you, lets make it simple. Lets go "fishing"., 


<img align=right width=400 src="/etc/img/cars.png">


[The Road to AI](https://arxiv.org/pdf/2212.11279.pdf) 
paints an exciting picture of modern AI, dominated by deep learning,
where those current techniques are the inevitable, inexorable result of
centuries are prior thought. 


It is a great paper, well worth your time. But it has its limitations.
In all that writing is one important concept:
how do you test such systems?
If you are a software engineer, you know that AI software is still
software. Software has bugs and
Anything with bugs needs to be tested.
How do you and your stakeholders[^stake] test a complex AI system? 


But before answering that, lets make sure we understand testing.
Firstly, Software has stakeholders so we need systems that can explain themselves
to anyone with a right, share, claim, or interest in that system. 
So we need someway that stakeholders can "drop in", from time to time,
and comment on the software that effects in. That process needs to support:
- _high-level explanations_ where the stakeholders may not understand much 
  of the low-level detail of the system; 
- _rapid testing_ since the stakeholders are typically busy to get back to their day job
- _periodic testing_ (where the stakeholder tests are interment, and separated by
    weeks or months. Ideally, something is learned from stakeholder testing in
    (say) January that can be applied automatically until the stakeholders
    return in (say) March.
- _incremental testing_ where, each time we return to retest,
    something learned from the past simplifies this next round of testing.


Secondly, testing is not just some process of adding check marks to a system.
Testing needs to verify that the system us built right
but it also needs to look further and validate that the right system is being built.
My own Ph.D. started out as a generic
testing mechanism. It took a while to realize
the obvious: testing "it" means being
able to exercise "it" so, properly designed, a generic test engine
is actually  a generic execution engine that can do many things[^abkl]
(and not just testing).
Anyone familiar with the rapid feedback cycles seen in modern software
development knows that "testing" is also about "exploring" ideas and
making them better.If w is not true, then we can get stuck...


- "Imagine if (the ancient greeks)  had developed robots... 
automating tasks at that time, 3,000
years ago, .... such as pottery and
weaving...  human labor would no longer be needed,
people would live lives of leisure...
stuck with the artifacts and production of the time.
To raise the quality of life substantially,
we can't build machines that merely substitute for human labor and
imitate it; <b>we must expand our capabilities and do new things</b>."    
-- [Erik Brynjolfsson](https://www.zdnet.com/article/ai-debate-3-everything-you-need-to-know-about-artificial-general-intelligence/)


[^abkl]: Tim Menzies,
[Applications of abduction: knowledge-level modelling](https://menzies.us/pdf/96abkl.pdf),
International Journal of Human-Computer Studies,
Volume 45, Issue 3,
1996,
Pages 305-335,
ISSN 1071-5819,
https://doi.org/10.1006/ijhc.1996.0054.
(https://www.sciencedirect.com/science/article/pii/S1071581996900543)
Abstract: A single inference procedure (abduction) can operationalise a wide variety of knowledge-level modelling problem solving methods; i.e. prediction, classification, explanation, tutoring, qualitative reasoning, planning, monitoring, set-covering diagnosis, consistency-based diagnosis, validation, and verification. This abductive approach offers a uniform view of different problem solving methods in the style proposed by Clancey and Breuker. Also, this adbuctive approach is easily extensible to validation; i.e. using this technique we can implement both inference tools and testing tools. Further, abduction can execute in vague and conflicting domains (which we believe occur very frequently). We therefore propose abduction as a framework for knowledge-level modelling.


The third thing we need to understand about testing is that while we can automate
billions of system inputs per second, all that is useless unless some oracle can tell us
how to assess those outputs.


XXXX oracle problem. metamorphic. large system, sample the least.


TESTED assumes that the best way to test something is to give it
to someone else, and watch them break it.  
- This is actually a core
principle of ethical programming.  
- Vance et al. [^Vance2015] argue that a
pre-condition for the accountability is the knowledge of an 
**external audience**, who could approve or disapprove of a system.  XXX other components as well


TESTED has many tools for such external audiences:


- Methods for looking beyond those boundaries (taken from cognitive
psychology); 
- Human-readable model generation methods that can
extract symbolic descriptions from training data (since that is
what humans need for explaining a system); 
- Cost-effect sampling
methods that let outsiders probe a system, looking for interesting
(or alarming) behavior.  
- Semi-supervised learners where algorithms
make conclusions based on a small sample of the total data space
(so humans are not overwhelmed with excessive questions).
- Operators for learning the boundaries of a system’s competency;
- Non-parametric tests for assuring that samples are truly different.


[^Vance2015]: Vance, Anthony, Paul Benjamin Lowry, and Dennis Eggett. 
  "Increasing Accountability Through User-Interface Design Artifacts." 
  MIS quarterly 39.2 (2015): 345-366.


[^Baltes22]: Baltes, S., Ralph, P. 
  [Sampling in software engineering research: a critical review and guidelines](https://arxiv.org/pdf/2002.07764.pdf);
  Empir Software Eng 27, 94 (2022);  https://doi.org/10.1007/s10664-021-10072-8.
	
[^Niu07]: Nan Niu, Steve M. Easterbrook: 
  [So, You Think You Know Others' Goals? A Repertory Grid Study](https://www.cse.msstate.edu/~niu/papers/SW07.pdf); 
  IEEE Softw. 24(2): 53-61 (2007) https://ieeexplore.ieee.org/document/4118651.


## Background


### The Data Pattern


Data Num Sym COLS, ROW

### The Rule patther
RANGE, RULE

### clustering = class

when u dont know what is going on, cluster then use those clsters for class

### imbalance

resample to balance

### Explanation


isnance-based, model-based


TESTED assumes that better
data mining algorithms are better at explaining their models to
humans. But is that always the case?


The obvious counter-argument is that if no human ever needs
to understand a model, then it does not need to be comprehensible. For example, a neural net could control the carburetor
of an internal combustion engine since that carburetor will never
dispute the model or ask for clarification of any of its reasoning.


On the other hand, if a model is to be used to persuade software
engineers to change what they are doing, it needs to be comprehensible so humans can debate the merits of its conclusions. Several
researchers demand that software analytics models needs to be
expressed in a simple way that is easy for software practitioners
to interpret. According to Kim et al. [^kim16], software analytics aim to obtain actionable insights from software artifacts
that help practitioners accomplish tasks related to software development, systems, and users. Other researchers argue that
for software vendors, managers, developers and users, such comprehensible insights are the core deliverable of software analytics.
Sawyer et al. comments that actionable insight is the key driver for
businesses to invest in data analytics initiatives [^saw13]. Accordingly,
much research focuses on the generation of simple models, or make
blackbox models more explainable, so that human engineers can
understand and appropriately trust the decisions made by software
analytics models.


Some researchers go further and warn that, for mission critical applications, we should never use opaque back-box models.
In 


XXX


LIME
